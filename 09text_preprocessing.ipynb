{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram 어절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from konlpy.corpus import kolaw\n",
    "\n",
    "corpus = kolaw.open(kolaw.fileids()[0]).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eojeol(text, n=2):\n",
    "    tokens = text.split()\n",
    "    ngram = list()\n",
    "    \n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngram.append(' '.join(tokens[i:i+n]))\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국헌법 유구한', '유구한 역사와', '역사와 전통에', '전통에 빛나는', '빛나는 우리', '우리 대한국민은', '대한국민은 3·1운동으로', '3·1운동으로 건립된', '건립된 대한민국임시정부의', '대한민국임시정부의 법통과', '법통과 불의에', '불의에 항거한', '항거한 4·19민주이념을', '4·19민주이념을 계승하고,', '계승하고, 조국의', '조국의 민주개혁과', '민주개혁과 평화적', '평화적 통일의', '통일의 사명에', '사명에 입각하여', '입각하여 정의·인도와', '정의·인도와 동포애로써', '동포애로써 민족의', '민족의 단결을', '단결을 공고히', '공고히 하고,', '하고, 모든', '모든 사회적', '사회적 폐습과', '폐습과 불의를', '불의를 타파하며,', '타파하며, 자율과', '자율과 조화를', '조화를 바탕으로', '바탕으로 자유민주적', '자유민주적 기본질서를', '기본질서를 더욱', '더욱 확고히', '확고히 하여', '하여 정치·경제·사회·문화의', '정치·경제·사회·문화의 모든', '모든 영역에', '영역에 있어서', '있어서 각인의', '각인의 기회를', '기회를 균등히', '균등히 하고,', '하고, 능력을', '능력을 최고도로', '최고도로 발휘하게', '발휘하게 하며,', '하며, 자유와', '자유와 권리에', '권리에 따르는', '따르는 책임과', '책임과 의무를', '의무를 완수하게', '완수하게 하여,', '하여, 안으로는', '안으로는 국민생활의', '국민생활의 균등한', '균등한 향상을', '향상을 기하고', '기하고 밖으로는', '밖으로는 항구적인', '항구적인 세계평화와', '세계평화와 인류공영에', '인류공영에 이바지함으로써', '이바지함으로써 우리들과', '우리들과 우리들의', '우리들의 자손의', '자손의 안전과', '안전과 자유와', '자유와 행복을', '행복을 영원히', '영원히 확보할', '확보할 것을', '것을 다짐하면서', '다짐하면서 1948년', '1948년 7월', '7월 12일에', '12일에 제정되고', '제정되고 8차에', '8차에 걸쳐', '걸쳐 개정된', '개정된 헌법을', '헌법을 이제', '이제 국회의', '국회의 의결을', '의결을 거쳐', '거쳐 국민투표에', '국민투표에 의하여', '의하여 개정한다.']\n"
     ]
    }
   ],
   "source": [
    "for _ in sent_tokenize(corpus):\n",
    "    print(eojeol(_))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram 음절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umjeol(text, n=2):\n",
    "    ngram = list()\n",
    "    \n",
    "    for i in range(len(text)-(n-1)):\n",
    "        ngram.append(''.join(text[i:i+n]))\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한', '한민', '민국', '국헌', '헌법', '법\\n', '\\n\\n', '\\n유', '유구', '구한', '한 ', ' 역', '역사', '사와', '와 ', ' 전', '전통', '통에', '에 ', ' 빛', '빛나', '나는', '는 ', ' 우', '우리', '리 ', ' 대', '대한', '한국', '국민', '민은', '은 ', ' 3', '3·', '·1', '1운', '운동', '동으', '으로', '로 ', ' 건', '건립', '립된', '된 ', ' 대', '대한', '한민', '민국', '국임', '임시', '시정', '정부', '부의', '의 ', ' 법', '법통', '통과', '과 ', ' 불', '불의', '의에', '에 ', ' 항', '항거', '거한', '한 ', ' 4', '4·', '·1', '19', '9민', '민주', '주이', '이념', '념을', '을 ', ' 계', '계승', '승하', '하고', '고,', ', ', ' 조', '조국', '국의', '의 ', ' 민', '민주', '주개', '개혁', '혁과', '과 ', ' 평', '평화', '화적', '적 ', ' 통', '통일', '일의', '의 ', ' 사', '사명', '명에', '에 ', ' 입', '입각', '각하', '하여', '여 ', ' 정', '정의', '의·', '·인', '인도', '도와', '와 ', ' 동', '동포', '포애', '애로', '로써', '써 ', ' 민', '민족', '족의', '의 ', ' 단', '단결', '결을', '을 ', ' 공', '공고', '고히', '히 ', ' 하', '하고', '고,', ', ', ' 모', '모든', '든 ', ' 사', '사회', '회적', '적 ', ' 폐', '폐습', '습과', '과 ', ' 불', '불의', '의를', '를 ', ' 타', '타파', '파하', '하며', '며,', ', ', ' 자', '자율', '율과', '과 ', ' 조', '조화', '화를', '를 ', ' 바', '바탕', '탕으', '으로', '로 ', ' 자', '자유', '유민', '민주', '주적', '적 ', ' 기', '기본', '본질', '질서', '서를', '를 ', ' 더', '더욱', '욱 ', ' 확', '확고', '고히', '히 ', ' 하', '하여', '여 ', ' 정', '정치', '치·', '·경', '경제', '제·', '·사', '사회', '회·', '·문', '문화', '화의', '의 ', ' 모', '모든', '든 ', ' 영', '영역', '역에', '에 ', ' 있', '있어', '어서', '서 ', ' 각', '각인', '인의', '의 ', ' 기', '기회', '회를', '를 ', ' 균', '균등', '등히', '히 ', ' 하', '하고', '고,', ', ', ' 능', '능력', '력을', '을 ', ' 최', '최고', '고도', '도로', '로 ', ' 발', '발휘', '휘하', '하게', '게 ', ' 하', '하며', '며,', ', ', ' 자', '자유', '유와', '와 ', ' 권', '권리', '리에', '에 ', ' 따', '따르', '르는', '는 ', ' 책', '책임', '임과', '과 ', ' 의', '의무', '무를', '를 ', ' 완', '완수', '수하', '하게', '게 ', ' 하', '하여', '여,', ', ', ' 안', '안으', '으로', '로는', '는 ', ' 국', '국민', '민생', '생활', '활의', '의 ', ' 균', '균등', '등한', '한 ', ' 향', '향상', '상을', '을 ', ' 기', '기하', '하고', '고 ', ' 밖', '밖으', '으로', '로는', '는 ', ' 항', '항구', '구적', '적인', '인 ', ' 세', '세계', '계평', '평화', '화와', '와 ', ' 인', '인류', '류공', '공영', '영에', '에 ', ' 이', '이바', '바지', '지함', '함으', '으로', '로써', '써 ', ' 우', '우리', '리들', '들과', '과 ', ' 우', '우리', '리들', '들의', '의 ', ' 자', '자손', '손의', '의 ', ' 안', '안전', '전과', '과 ', ' 자', '자유', '유와', '와 ', ' 행', '행복', '복을', '을 ', ' 영', '영원', '원히', '히 ', ' 확', '확보', '보할', '할 ', ' 것', '것을', '을 ', ' 다', '다짐', '짐하', '하면', '면서', '서 ', ' 1', '19', '94', '48', '8년', '년 ', ' 7', '7월', '월 ', ' 1', '12', '2일', '일에', '에 ', ' 제', '제정', '정되', '되고', '고 ', ' 8', '8차', '차에', '에 ', ' 걸', '걸쳐', '쳐 ', ' 개', '개정', '정된', '된 ', ' 헌', '헌법', '법을', '을 ', ' 이', '이제', '제 ', ' 국', '국회', '회의', '의 ', ' 의', '의결', '결을', '을 ', ' 거', '거쳐', '쳐 ', ' 국', '국민', '민투', '투표', '표에', '에 ', ' 의', '의하', '하여', '여 ', ' 개', '개정', '정한', '한다', '다.']\n"
     ]
    }
   ],
   "source": [
    "for _ in sent_tokenize(corpus):\n",
    "    print(umjeol(_))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'수 있다.': 56, '법률이 정하는': 48, '정하는 바에': 37, '바에 의하여': 36, '법률로 정한다.': 28, '모든 국민은': 23, '수 없다.': 20, '① 모든': 14, '사항은 법률로': 14, '의무를 진다.': 11, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = eojeol(corpus) # => 2어절 패턴\n",
    "bigramText = Text(bigram)\n",
    "bigramText.vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WPM (Word Piece Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ WPM: 하나의 단어를 내부 단어(Subword Unit)들로 분리하는 단어 분리 모델\n",
    "+ BPE: Byte Pair Encoding (Digram Coding) -> Simple form of data compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte Pair Encoding\n",
    "('어절':빈도), ('어절':빈도), ('어절':빈도)\n",
    "low:5, lowest:2\n",
    "l o w </w>\n",
    "l o w e s t </w>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTerm(term):\n",
    "    result = list()\n",
    "    for token in term.split():\n",
    "        result.append(' '.join(list(term) + ['</w>']))\n",
    "    return '_'.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def ngram(data, n=2):\n",
    "    result = defaultdict(int)\n",
    "    \n",
    "    for term, freq in data.items():\n",
    "        tokens = term.split()\n",
    "        for i in range(len(tokens) - (n-1)):\n",
    "            result[' '.join(tokens[i:i+n])] += freq\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mergeNgram(maxKey, data):\n",
    "    newData = dict()\n",
    "    \n",
    "    for term, freq in data.items():\n",
    "        newKey = re.sub(maxKey, maxKey.replace(' ',''), term)\n",
    "        newData[newKey] = freq\n",
    "    \n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'low </w>': 5, 'low e s t </w>': 2, 'ne w er</w>': 6, 'w i d er</w>': 3}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    splitTerm('low'):5,\n",
    "    splitTerm('lowest'):2,\n",
    "    splitTerm('newer'):6,\n",
    "    splitTerm('wider'):3\n",
    "}\n",
    "for _ in range(5):\n",
    "    bigram = ngram(data)\n",
    "    maxKey = max(bigram, key=bigram.get)\n",
    "    data = mergeNgram(maxKey, data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'd like to learn more somthing.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization\n",
    "# 1. 대소문자 통합(소문자)\n",
    "# 2. 구두점 처리(I'd, I'm) => tokenizing\n",
    "#    => 대안 : 형태소 분석기\n",
    "# 3. 불용어(stopwords) 처리\n",
    "sentence = \"I'd like to learn more somthing.\"\n",
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구두점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Id like to learn more somthing'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.sub('[{0}]'.format(re.escape(punctuation)), '', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id like to learn more somthing'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('[{0}]'.format(re.escape(punctuation)))\n",
    "pattern.sub('', sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i 'd like to learn more somthing .\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "' '.join(word_tokenize(sentence.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i d l i k e to learn'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\s{2,}', ' ', 'i     d l  i k e to learn')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords(불용어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.open('english').read()\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful\n",
      "Skipped\n",
      "better\n",
      "Skipped\n",
      "ugly\n",
      "Skipped\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love you'\n",
    "sentence = 'Beautiful is better than ugly.'\n",
    "\n",
    "for _ in word_tokenize(sentence.lower()): # 1번\n",
    "    if pattern.sub('', _) in stop: # 2번 with stopwords\n",
    "# for _ in pattern.sub('', sentence.lower()).split():\n",
    "#     if _ in stop:\n",
    "        print('Skipped')\n",
    "    else:\n",
    "        print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69791"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "corpus = gutenberg.open('austen-emma.txt').read()\n",
    "\n",
    "# len(word_tokenize(corpus)) # 19만개 -> 6.9만개\n",
    "\n",
    "words = list()\n",
    "for _ in word_tokenize(corpus.lower()): # 1번\n",
    "    if pattern.sub('', _) not in stop: # 2번 with stopwords\n",
    "        words.append(_)\n",
    "        \n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글에 불용어, stopwords 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,\n",
       " ['어머니', '는', '짜장면', '이', '싫다', '고', '하셨', '어'],\n",
       " 4,\n",
       " ['어머니', '짜장면', '싫다', '하셨'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korstop = ['은', '는', '이', '가', '고', '을', '를', '께서', '에게', '어']\n",
    "\n",
    "sentence = '어머니 는 짜장면 이 싫다 고 하셨 어.'\n",
    "sentence = pattern.sub('', sentence)\n",
    "\n",
    "len(word_tokenize(sentence)), word_tokenize(sentence), \\\n",
    "len([_ for _ in word_tokenize(sentence) if _ not in korstop]), [_ for _ in word_tokenize(sentence) if _ not in korstop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "learn\n",
      "more\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['like', 'learn', 'more']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I'd like to learn more something.\"\n",
    "# 길이 정규화 => 특정 길이(너무 짧거나 너무 길거나)\n",
    "for _ in pattern.sub('', sentence.lower()).split():\n",
    "    if 2 < len(_) < 6:\n",
    "        print(_)\n",
    "        \n",
    "minimum = 3\n",
    "maximum = 5\n",
    "pattern2 = re.compile(r'\\b\\w{%d,%d}\\b' % (minimum, maximum))\n",
    "pattern2.findall(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어머니\n",
      "짜장면\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['어머니', '짜장면']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '어머니 는 짜장면 이 싫다 고 하셨 어.'\n",
    "\n",
    "for _ in pattern.sub('', sentence.lower()).split():\n",
    "    if 2 < len(_) < 6:\n",
    "        print(_)\n",
    "\n",
    "minimum = 3\n",
    "maximum = 5\n",
    "pattern2 = re.compile(r'\\b\\w{%d,%d}\\b' % (minimum, maximum))\n",
    "pattern2.findall(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "\n",
    "obj = Text(word_tokenize(pattern.sub('', sentence.lower())))\n",
    "for _ in obj.vocab():\n",
    "    if 1 < obj.vocab().get(_) < 3:\n",
    "        print(_, obj.vocab().get(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비속어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eojeol(text, n=2):\n",
    "    tokens = text.split() \n",
    "    ngram = list()\n",
    "    \n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngram.append(' '.join(tokens[i:i+n])) \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umjeol(text, n=2):\n",
    "    ngram = list()\n",
    "    \n",
    "    for i in range(len(text)-(n-1)):\n",
    "        ngram.append(''.join(text[i:i+n])) \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def ngram(tokens, n=2):\n",
    "    result = defaultdict(int)\n",
    "    for k, v in tokens.items():\n",
    "        term = k.split()        \n",
    "        for i in range(len(term) - n + 1):\n",
    "            ngram = (term[i], term[i+1])         \n",
    "            if ngram in result.keys():\n",
    "                result[ngram] += v\n",
    "            else:\n",
    "                result[ngram] = v\n",
    "                \n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeNgram(maxKey, data):\n",
    "    result = defaultdict(int)   \n",
    "    token = ' '.join(maxKey)\n",
    "    pattern = re.compile(r'(?!=\\S)' + token + '(?!\\S)') # S : whitespace    \n",
    "    for k, v in data.items():\n",
    "        new = pattern.sub(''.join(maxKey), k)\n",
    "        result[new] = v\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTerms(term):\n",
    "    termList = term.split()\n",
    "    result = []    \n",
    "    for t in termList:\n",
    "        result.append(' '.join(['<w>']+list(t)+['</w>']))\n",
    "        \n",
    "    return ' _ '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 시발짜증나네\n"
     ]
    }
   ],
   "source": [
    "stop = ['시발']\n",
    "\n",
    "sentence = '시발 시발짜증나네'\n",
    "result = list()\n",
    "\n",
    "# [_ for _ in sentence.split() if _ not in stop]\n",
    "\n",
    "for term in sentence.split():\n",
    "    if term in stop:\n",
    "        result.append('*' * len(term))\n",
    "    else:\n",
    "        result.append(term)\n",
    "        \n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** ** ** **** *****\n"
     ]
    }
   ],
   "source": [
    "stop = ['시발', '씨발']\n",
    "\n",
    "sentence = '씨발짜증나네 시발 씨발 씨발12 시발123'\n",
    "result = []\n",
    "\n",
    "for term in sentence.split():\n",
    "    if term in stop:\n",
    "        result.append('*' * len(term))\n",
    "    else:\n",
    "        flag = False    \n",
    "        for token in umjeol(term):\n",
    "            if token in stop:\n",
    "                flag = True\n",
    "                \n",
    "        if flag:\n",
    "            result.append('*' * len(term))        \n",
    "        else:\n",
    "            result.append(term)\n",
    "        \n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 시~발\n"
     ]
    }
   ],
   "source": [
    "stop = ['시발', '씨발']\n",
    "\n",
    "sentence = '시123발 시~발'\n",
    "result = []\n",
    "\n",
    "pattern = re.compile(r'\\B[0-9]+\\B')\n",
    "# pattern = re.compile(r'[0-9]+')\n",
    "sentence = pattern.sub('',sentence)\n",
    "\n",
    "for term in sentence.split():\n",
    "    if term in stop:\n",
    "        result.append('*' * len(term))\n",
    "    else:\n",
    "        flag = False    \n",
    "        for token in umjeol(term):\n",
    "            if token in stop:\n",
    "                flag = True\n",
    "                \n",
    "        if flag:\n",
    "            result.append('*' * len(term))        \n",
    "        else:\n",
    "            result.append(term)\n",
    "        \n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** ***** *****\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    splitTerms('시발'):10, # <w> 시 발 </w>\n",
    "    splitTerms('시~발'):2, # <w> ? </w> <w> 시 발 </w>\n",
    "    splitTerms('시!발'):2,\n",
    "    splitTerms('시1발'):2\n",
    "}\n",
    "\n",
    "for i in range(2):\n",
    "    pattern = ngram(data)\n",
    "    maxKey = max(pattern, key = lambda x:pattern[x]) \n",
    "    data = mergeNgram(maxKey, data) \n",
    "\n",
    "pattern = ngram(data)\n",
    "maxKey = max(pattern, key = lambda x:pattern[x])\n",
    "\n",
    "sentence = '시발 시1~!발 시~!~발'\n",
    "repattern = re.compile(maxKey[0] + '.*' + maxKey[1])\n",
    "\n",
    "result = []\n",
    "for token in sentence.split():\n",
    "    if repattern.search('<w>{0}</w>'.format(token)):\n",
    "        result.append('*'*len(token))\n",
    "    else:\n",
    "        result.append(token)\n",
    "\n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "ma = Kkma()\n",
    "# ma.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.help import brown_tagset\n",
    "# brown_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ https://nlp.stanford.edu/software/tagger.html#Download\n",
    "+ https://nlp.stanford.edu/software/CRF-NER.html#Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.help import brown_tagset\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "model = r\"C:\\Users\\Yang Saewon\\Downloads\\stanford-postagger-full-2018-10-16\\stanford-postagger-full-2018-10-16\\models\\english-bidirectional-distsim.tagger\"\n",
    "parser = r\"C:\\Users\\Yang Saewon\\Downloads\\stanford-postagger-full-2018-10-16\\stanford-postagger-full-2018-10-16\\stanford-postagger-3.9.2.jar\"\n",
    "pos = StanfordPOSTagger(model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = 'C:/Program Files/Java/jdk1.8.0_201/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'NNP'),\n",
       " ('Emma', 'NNP'),\n",
       " ('by', 'IN'),\n",
       " ('Jane', 'NNP'),\n",
       " ('Austen', 'NNP'),\n",
       " ('1816', 'CD'),\n",
       " (']', 'NNP'),\n",
       " ('VOLUME', 'NNP'),\n",
       " ('I', 'NNP'),\n",
       " ('CHAPTER', 'NNP'),\n",
       " ('I', 'NNP'),\n",
       " ('Emma', 'NNP'),\n",
       " ('Woodhouse', 'NNP'),\n",
       " (',', ','),\n",
       " ('handsome', 'JJ'),\n",
       " (',', ','),\n",
       " ('clever', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('rich', 'JJ'),\n",
       " (',', ','),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('comfortable', 'JJ'),\n",
       " ('home', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('happy', 'JJ'),\n",
       " ('disposition', 'NN'),\n",
       " (',', ','),\n",
       " ('seemed', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('unite', 'VB'),\n",
       " ('some', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('best', 'JJS'),\n",
       " ('blessings', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('existence', 'NN'),\n",
       " (';', ':'),\n",
       " ('and', 'CC'),\n",
       " ('had', 'VBD'),\n",
       " ('lived', 'VBN'),\n",
       " ('nearly', 'RB'),\n",
       " ('twenty-one', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('very', 'RB'),\n",
       " ('little', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('distress', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('vex', 'VB'),\n",
       " ('her', 'PRP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "pos.tag(word_tokenize(sent_tokenize(corpus)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
