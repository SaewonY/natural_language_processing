{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. requests를 이용 뉴스 사이트 가져오기 (https://news.nate.com/, https://media.daum.net/) <br>\n",
    "3. 각 카테고리 (예) 정치/경제/사회/문화/세계/IT) <br>\n",
    "4. 카테고리별 이름 추출 <br>\n",
    "5. 각 뉴스로 이동 <br>\n",
    "6. 각 뉴스의 본문 카테고리 찾기 <br>\n",
    "7. 본문의 텍스트만 추출하여 저장 <br\n",
    "File Name : 카테고리-aid.text ex) 정치-20190510190906476.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "header = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDownload(url, param = None, retries = 3):\n",
    "    resp = None\n",
    "    try:\n",
    "        resp = requests.get(url, params = param, headers = header)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if 500 <= resp.status_code < 600 and retries > 0:\n",
    "            print('Retries : {0}'.format(retries))\n",
    "            return getDownload(url, param, retries -1)\n",
    "        else:\n",
    "            print(resp.status_code)\n",
    "            print(resp.reason)\n",
    "            print(resp.request.headers)\n",
    "            \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음 뉴스기사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://media.daum.net/\"\n",
    "html = getDownload(url)\n",
    "dom = BeautifulSoup(html.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 정규표현식\n",
    "def clean_text(text):\n",
    "                cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "                cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]', '', cleaned_text)\n",
    "                cleaned_text = re.sub('\\n', '', cleaned_text)\n",
    "                return cleaned_text\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 홈에서 뉴스 카테고리 선택\n",
    "for _ in dom.select(\".gnb_comm > li > a\")[1:6]:\n",
    "#     print(_.text) # 카테고리명 출력\n",
    "    suburl = requests.compat.urljoin(url, _[\"href\"])\n",
    "    category = _.text\n",
    "    \n",
    "    html = getDownload(suburl)\n",
    "    newdom = BeautifulSoup(html.text, \"lxml\")\n",
    "            \n",
    "    for _ in newdom.select(\"strong.tit_thumb > a.link_txt\"):\n",
    "#         print(_.text.strip())\n",
    "#         print(_[\"href\"])\n",
    "        \n",
    "        subsuburl = requests.compat.urljoin(suburl, _[\"href\"])\n",
    "        html = getDownload(subsuburl)\n",
    "        newdom2 = BeautifulSoup(html.text, \"lxml\")\n",
    "        \n",
    "        #파일 저장할 경로\n",
    "        path = \"C:\\\\Users\\\\Yang Saewon\\\\Desktop\\\\project\\\\github\\\\natural_language_processing\\\\data\\\\\"\n",
    "        filename = _[\"href\"].split('/')[-1] \n",
    "        \n",
    "        \n",
    "        # 해당 뉴스 본문 가져오기\n",
    "        for _ in newdom2.select(\"div.news_view\"):\n",
    "            news_content_text = _.text.strip()\n",
    "#             print(news_content_text)\n",
    "            \n",
    "            news_content_text = clean_text(news_content_text)\n",
    "\n",
    "            \n",
    "            with open(path + category + '_' + filename + \".txt\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(news_content_text)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이트 뉴스기사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.nate.com/\"\n",
    "html = getDownload(url)\n",
    "dom = BeautifulSoup(html.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in dom.select(\"[class^='news0'] > a\")[2:7]: \n",
    "#     print(_.text) # 카테고리명 출력\n",
    "    category = _.text\n",
    "    \n",
    "    suburl = requests.compat.urljoin(url, _[\"href\"])\n",
    "\n",
    "    html = getDownload(suburl)\n",
    "    newdom = BeautifulSoup(html.text, \"lxml\")\n",
    "    \n",
    "    for _ in newdom.select(\"ul.mduList1 > li > a\"):\n",
    "        \n",
    "#         print(_.text.strip())\n",
    "#         print(_[\"href\"])\n",
    "                \n",
    "        subsuburl = requests.compat.urljoin(suburl, _[\"href\"])\n",
    "        html = getDownload(subsuburl)\n",
    "        newdom2 = BeautifulSoup(html.text, \"lxml\")\n",
    "        \n",
    "        \n",
    "        #파일 저장할 경로\n",
    "        path = \"C:\\\\Users\\\\Yang Saewon\\\\Desktop\\\\project\\\\github\\\\natural_language_processing\\\\data\\\\\"\n",
    "        filename = _[\"href\"].split('/')[-1][4:13]\n",
    "        \n",
    "        \n",
    "        # 해당 뉴스 본문 가져오기\n",
    "        for _ in newdom2.find_all('div',{'id':'realArtcContents'}):\n",
    "            \n",
    "            news_content_text = _.text.strip()            \n",
    "            news_content_text = clean_text(news_content_text)\n",
    "\n",
    "            with open(path + category + '_' + filename + \".txt\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(news_content_text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![123](https://user-images.githubusercontent.com/40786348/57563117-ec865b80-73d4-11e9-980e-ba005fdd156a.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
