# **http 요청 (request)와 응답(reponse)**

<br>

웹 브라우저(클라이언트)로 웹 사이트(서버)에 접속할 때 일어나는 일을 순서대로 생각해 보면 다음과 같다.

1. 사용자가 웹 브라우저의 주소창에 주소를 입력한다.
2. 웹 브라우저는 요청 메시지를 작성해 웹 서버에 보낸다. (요청)
3. 요청 메시지가 인터넷의 복잡한 통신망을 거쳐 웹 서버에 전달된다. (요청 완료)
4. 웹 서버는 요청받은 정보를 요청자에게 보낸다. (응답)
5. 응답 메시지가 인터넷의 복잡한 통신망을 거쳐 웹 브라우저에 전달된다. (응답 완료)
6. 웹 브라우저가 사용자에게 정보를 출력해준다.

<br>

이때 웹 서버와 웹브라우저가 요청하고 응답받는 과정은 http protocol이라는 약속 방식에 따라 진행된다. 따라서 크롤러를 만들기 위해서 가장 먼저 해야할 일은 웹 서버에 웹 페이지를 요청하고 응답받을 수 있게 하는 것이다.

<br>

파이썬으로 웹 요청을 수행하는 것은 여러분이 평소 웹 브라우저로 웹 사이트에 접속하는 것과 똑같은 작업이다. 차이가 있다면, 주소를 입력하는 곳이 주소창이 아니라 함수의 매개변수라는 것 정도다.

<br>

http 통신을 주고 받는 과정은 매우 간단하다. (Request 객체를 만들고 urlopen method를 호출하기만 하면 된다)

<br>

파이썬은 URL과 웹 요청에 관련된 모듈들을 urllib(URL 관련 라이브러리라는 의미)이라는 패키지로 묶어 제공한다. 이 패키지에 담긴 모듈 중에서 **주소를 해석해주는 urllib.parse 모듈**과 **특정 주소에 HTTP 요청을 수행하는 urllib.request 모듈**이 주로 쓰인다.


urllib.request module을 import한 이후, urllib.request.urlopen(요청할url).read().decode('utf-8') 을 하면 웹 요청을 보낼 수 있다.

풀어서 설명하면,
1. urllib.request.urlopen() 함수는 웹 서버에 정보를 요청한 후, 돌려받은 응답을 저장하여 ‘응답 객체(HTTPResponse)’를 반환한다.
2. 응답 객체의 read() 메서드를 실행하여 서버가 전달한 데이터를 바이트 배열로 읽어들인다.
3. 바이트 배열은 십육진수로 이루어진 수열이기 때문에 사람의 눈으로 읽기가 어렵다. 웹 서버가 보낸 문서는 텍스트 데이터일 것이므로 이것을 텍스트 형식으로 변환한다. 바이트 배열의 decode() 메서드를 실행하여 ‘UTF-8’(유니코드 부호화 형식의 한 종류)로 해석하면 문자열이 반환된다.


<br>
<br>

요청한 결과로 서버가 응답해 준 텍스트를 print()해보면 뭔가 이해하기 힘든 복잡한 텍스트가 화면을 가득 메운다. 이것은 HTML(HyperText Markup Language)이라는 언어로 작성된 마크업 문서인데 우리가 웹 브라우저로 동일한 URL에 접속했을 때 전달받는 문서와 동일한 문서이다. 웹 브라우저는 이런 마크업 문서를 사람이 보기 좋게 출력해준다. 

<br>

하지만 파이썬으로 읽으려 한다면 그 목적은 정보를 분석, 재가공하여 유용하게 쓰고자함일 것이다. HTML 문서는 프로그램에서 해석하고 처리하기 까다롭다. 다행히 웹에는 JSON처럼 좀 더 프로그램에서 다루기 좋은 데이터도 제공된다.

<br>

# 웹 API 활용하기

웹에서는 다양한 기관과 회사가 프로그래밍에 필요한 정보를 제공하고 있다. 이런 서비스를 웹 API(Application Programming Interface)라고 한다. 웹 API에서 제공하는 데이터는 대부분 (HTML이 아니라) JSON 또는 XML 형식이어서 프로그래밍에 활용하기 용이하다. 웹 API로 제공되는 정보의 종류도 매우 다양한데, 다음은 몇 가지만 예를 들어본 것이다.

* 대한민국 공공기관 정보: 공공데이터포털(data.go.kr)에서 API 사용허가를 얻은 후 열람. 기상청에서 제공하는 기상 예보 정보가 특히 많이 사용된다.
* 세계 다국어 사전: Glosbe(glosbe.com)에서 사용신청 없이 자유롭게 사용
* 세계 영화 정보: TMDb(themoviedb.org)에 계정 생성 후 사용
* 페이스북 사용자·게시물 정보: 페이스북 개발자 사이트(developers.facebook.com)에서 개발자 계정 등록 후 페이스북 그래프 API 열람
* 텔레그램 메신저의 채팅 봇 조작: 텔레그램 계정으로 봇 계정 등록 후, 텔레그램 봇 API(https://core.telegram.org/bots/api) 사이트의 문서를 참고하여 이용
* 등등 여러가지가 있다.

<br>

## URL 다루기

URL은 인터넷 공간에 존재하는 자원을 가리키기 위한 절대 주소다. 다음과 같은 양식을 가지고 있다. 
<br>
**프로토콜://계정:패스워드@호스트:포트번호/하위경로?질의문#색인**
<br>
(이 양식에서 가장 자주 사용되는 요소는 프로토콜, 호스트, 하위 경로. 그 외는 자주 생략된다)

<br>

* 프로토콜: 자원에 접근하기 위한 통신 방법, 웹에서는 http와 https가 사용된다. 참고로 https는 http에 SSL이라는 암,복호화 단계를 적용하여 보안 통신을 수행하는 프로토콜이다.
* 호스트: 자원이 위치한 네트워크(또는 컴퓨터)의 도메인 주소 또는 IP 주소
* 하위 경로: 한 호스트는 여러 개의 자원을 제공할 수 있다. 그 하위 자원을 가리키기 위해 호스트 이름 뒤에 /wiki/Python_(programming_language)와 같이 표기한다.

<br>

### URL 분할, 수정, 재결합

파이썬에서 URL 조작시에는 urllib.parse 모듈을 사용한다. 이 모듈의 함수 urllib.parse.urlsplit()을 이용하면 URL을 여러 부분으로 나눌 수 있다.

<br>

### 퍼센트 인코딩

URL에 사용할 수 있는 문자는 영문자, 숫자, 몇몇 기호 뿐이다. 그 밖의 문자(한글, 한자, 특수문자 등)는 사용할 수 없다. 그래서 파이썬 문서를 가리키는 https://ko.wikipedia.org/wiki/파이썬 URL을 파이썬에서 요청하면 오류가 발생한다.

URL에 한글이 섞여 있으면 오류가 발생하기 때문에 아스키 코드가 아닌 문자들을 퍼센트 인코딩(percent encoding) 해줘야 한다. 우리가 사용하는 웹 브라우저는 퍼센트 인코딩을 자동으로 수행해준다. 파이썬에서는 urllib.parse.quote() 함수로 텍스트에 퍼센트 인코딩을 적용해 줄 수 있다.

거꾸로 되돌릴 때는 urllib.parse.unquote() 함수를 사용하면 된다.

<br>

## 웹 서버 프로그램의 실행 과정

프로그램과 함수가 입력과 출력 사이에서 보이지 않는 복잡한 중간 처리 과정을 거쳐 나오는데, 똑같이 웹 서버 프로그램도 요청과 응답 사이에 여러 단계의 실행 과정이 있다.

* 수신 대기(listen): 클라이언트의 요청이 오기를 기다린다.
* 중계(route): 요청을 받으면, 요청 메시지(URL, 메서드 등)를 해석하여 그에 해당하는 가능(함수)를 호출한다.
* 실행: 중계 과정에서 호출된 기능을 실제로 처리한다. 이 과정에서 데이터베이스 시스템과 같은 프로그램 외부의 자원을 활용하기도 한다.
* 출력 결과 가공(render): 실행된 결과를 클라이언트에게 제공하기 위한 특정 형식으로 가공한다. 이 과정에서 템플릿(틀을 이용한 가공) 도구를 활용하기도 한다.
* 응답: 실행된 결과를 클라이언트에게 되돌려준다.

이 과정을 프로그래머가 직접 구현하기는 힘들다. 실무에서는 웹 서버의 전체 기능을 미리 만들어 제공하는 **웹 프레임워크(web framework)**라는 라이브러리를 사용할 때가 많다.
파이썬을 위한 웹 프레임워크가 많이 있고 그 중에서 '장고(Django)'와 '플라스크(Flask)'가 가장 인기가 많다.


### reference: 'https://python.bakyeono.net/chapter-11-5.html'


<br>
<br>
<br>
<br>

# 4.30 수업 note

코드 참고: https://lms.koipa.or.kr/static/uploads/lectures/13/20190430%28%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B3%A0%EA%B8%89%EB%B0%98%20%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%29.html

<br>
<br>

![web](https://user-images.githubusercontent.com/40786348/57015263-de0aa800-6c4e-11e9-953d-6d198e269b2a.PNG)

<br>


<br>

* 특정 게시판의 정보가 필요할 경우 게시판에 있는 인덱스 파라미터 싹 다 긁으면 결국 db에 있는 것들을 다 가지고 올 수 있는 것이라 볼 수 있다.

<br>

* 일반적으로 요청을 보낸다 하면 request를 보낸다는 것. header를 분석해낼 수 있다. **status code**를 가지고 요청이 잘됐는지 확인할 수 있다.

<br>
<br>
<br>

![index](https://user-images.githubusercontent.com/40786348/57015281-f8448600-6c4e-11e9-89a5-13a5318d07e4.PNG)

![request](https://user-images.githubusercontent.com/40786348/57015283-fed2fd80-6c4e-11e9-9683-b7f4c5b44bf9.PNG)

![method](https://user-images.githubusercontent.com/40786348/57015287-085c6580-6c4f-11e9-8146-cae71093107d.PNG)

get과 post 두개를 기억하자. 이외에도 많으나 나중에 확인해볼 것.

<br>

내가 원하는 데이터를 자동으로 만들 수 있는 crawler, spyder를 만들고자 한다.

그전에 법적인 이슈를 먼저 짚고 나가야 한다.

<br>

# Legal Issues

### **Opt-in vs Opt-out**

1. Opt-in 정보수집을 명시적으로 동의할 때에만 정보수집 가능하다; Whitelist
2. Opt-out 정보수집을 명시적으로 거부할 때에만 정보수집 중단; Blacklist

* **퍼가지 말라고 명시적으로 나와있는 경우에 가져가면 불법이다. 개인 목적으로 쓰는건 상관 없지만 사업 목적에서 이용하는 것은 불법의 소지가 있다.**

<br>

#### 크롤링이 합법이다 vs 불법이다

1. 합법이다 주장: 항공권 가격 비교 서비스는 불법이 아니다. 공공의 이익. 하지만 여기서 수익이 나는 경우는 다르다.

2. 불법이다 주장: 우리가 접속하는건 서버지만 실제 중요한 데이터는 db에 존재할 것이다. 그 사람들의 고유한 자산이기 때문에 침해하면 안된다. 보통 자신들의 회원들에게만 서비스를 제공하기 때문.

<br>

우리가 만들 봇은 마치 회원인것 마냥 db에 접속할 수 있다. 이로 인해서 서버의 트래픽이 엄청 증가할 수 있다. 우리는 봇을 통해 일초에 수십 페이지를 긁을 수 있다. 사이트 운영하는 사람들 입장에서는 트래픽을 많이 잡아먹는 일이다. 마치 디도스 공격처럼 응용 가능하다.

<br>

많은 사례가 있다. 대표적 사례가 잡코리아, 사람인
사람인이 잡코리아에 거액의 소송비를 내줬다.
사람인 측이 네이버를 통해 들어갔기 때문에 

<br>
<br>
<br>


![이용방침](https://user-images.githubusercontent.com/40786348/57015296-17431800-6c4f-11e9-8e2a-c07f84009eb8.PNG)

* 위와 같이 사이트 하단의 이용방침에 보면 크롤링을 거부한다고 명시되어 있다

<br>
<br>

![robots_txt](https://user-images.githubusercontent.com/40786348/57015299-20cc8000-6c4f-11e9-82ac-25efe3d65c98.PNG)


* 또한 robots.txt는 도메인 뒤에 연결해서 치면 나온다. 대부분의 기업들은 이 텍스트 file에 크롤링은 허용할건지 안할건지에 대해서 명시해 놓는다. 하지만 대부분 국내 중소 기업들은 해놓지 않은 경우도 많다. 이 말은 즉슨, **마음대로 정보를 수집해가도 불법이 아니라는 말**이다.

<br>
<br>

![data](https://user-images.githubusercontent.com/40786348/57015315-3346b980-6c4f-11e9-8cf5-b0ee0f77978b.PNG)

* 크롤링 할 시 생각을 미리 해야할 것들. 어떤 데이터를 긁어올 것인가?

<br>
<br>

![tips](https://user-images.githubusercontent.com/40786348/57015317-39d53100-6c4f-11e9-8201-62511ead6232.PNG)

* builtwith를 사용해서 웹이 사용한 기술을 볼 수 있다.
* whois를 통해서 도메인 info를 가져올 수 있다.





